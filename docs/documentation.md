# ğŸ“‚ docs/README.md - DocumentaciÃ³n del Proyecto
"""
# â˜ï¸ AnÃ¡lisis de Datos en Azure con Databricks y SQL Server

## ğŸ“Œ DescripciÃ³n del Proyecto

Este proyecto implementa una **infraestructura de datos en Azure** utilizando **Databricks, SQL Server y almacenamiento en formato Parquet y JSON**. Se centra en la **automatizaciÃ³n de ingestiÃ³n, transformaciÃ³n y anÃ¡lisis de datos** con capacidades de anÃ¡lisis avanzado. Incluye:

- **Carga de archivos CSV, JSON y Parquet en Azure Data Lake.**
- **Procesamiento de datos con Spark en Databricks.**
- **Filtrado dinÃ¡mico de fechas basado en fecha proceso.**
- **Modelado y consultas en SQL Server con datos transformados.**
- **AnÃ¡lisis estadÃ­stico y modelos predictivos aplicados en PySpark.**

## ğŸ“‚ Estructura del Proyecto
```
ğŸ“ Azure_Databricks_Analytics
â”‚â”€â”€ ğŸ“‚ src/
â”‚   â”‚â”€â”€ ingest_csv_json_parquet.py  # Carga de datos en Azure Data Lake
â”‚   â”‚â”€â”€ transform_data_databricks.py  # Transformaciones con Spark en Databricks
â”‚   â”‚â”€â”€ filter_by_date.py  # Filtrado de datos por fecha proceso
â”‚   â”‚â”€â”€ analytics_pyspark.py  # Modelos analÃ­ticos en PySpark
â”‚â”€â”€ ğŸ“‚ tests/
â”‚   â”‚â”€â”€ test_ingest.py  # Pruebas de carga de datos
â”‚   â”‚â”€â”€ test_filter.py  # Pruebas de filtrado de datos
â”‚   â”‚â”€â”€ test_transform.py  # Pruebas de transformaciÃ³n de datos en Databricks
â”‚   â”‚â”€â”€ test_analytics.py  # Pruebas de anÃ¡lisis en PySpark
â”‚â”€â”€ ğŸ“‚ docs/
â”‚   â”‚â”€â”€ README.md  # DocumentaciÃ³n del proyecto
```

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

1. **Clona este repositorio:**
   ```sh
   git clone https://github.com/emmacm20-lab/Azure_Databricks_Analytics.git
   ```
2. **Carga los archivos en Azure Data Lake:**
   ```sh
   python src/ingest_csv_json_parquet.py
   ```
3. **Ejecuta la transformaciÃ³n de datos en Databricks:**
   ```sh
   python src/transform_data_databricks.py
   ```
4. **Realiza el anÃ¡lisis de datos con PySpark:**
   ```sh
   python src/analytics_pyspark.py
   ```

## ğŸ“© Flujo de Trabajo
1. **IngestiÃ³n de Datos**: Se cargan archivos CSV, JSON y Parquet en Azure Data Lake.
2. **Procesamiento en Databricks**: TransformaciÃ³n y limpieza con Spark.
3. **Filtrado DinÃ¡mico**: SelecciÃ³n de datos basada en fecha proceso.
4. **Almacenamiento y Consulta**: Datos cargados en SQL Server y analizados en Databricks.
5. **AnÃ¡lisis Predictivo**: Modelos estadÃ­sticos en PySpark para generaciÃ³n de insights.

## ğŸ›  TecnologÃ­as Utilizadas
- **Azure Databricks**: Procesamiento y anÃ¡lisis de datos con Spark.
- **Azure Data Lake**: Almacenamiento de archivos estructurados y semiestructurados.
- **SQL Server**: ConsolidaciÃ³n de datos y consultas analÃ­ticas.
- **PySpark**: Modelos analÃ­ticos y filtrado dinÃ¡mico.
- **Linux & Bash**: AutomatizaciÃ³n de procesos en entornos cloud.

## ğŸ“ˆ Resultados Esperados
- ğŸ“Œ **AutomatizaciÃ³n de ingestiÃ³n y procesamiento de datos en Azure.**
- ğŸ“Œ **OptimizaciÃ³n de consultas y modelos predictivos en Databricks.**
- ğŸ“Œ **IntegraciÃ³n fluida entre datos estructurados y no estructurados.**
- ğŸ“Œ **Mayor eficiencia en anÃ¡lisis de datos para toma de decisiones.**

## ğŸ§ª Pruebas
El proyecto incluye pruebas en Python para validar la carga, filtrado y procesamiento de datos.
1. **EjecuciÃ³n de pruebas:**
   ```sh
   python -m unittest discover tests/
   ```

## ğŸ“¬ Contacto
Para consultas o sugerencias, contÃ¡ctame en [emmanuel.cmora20@gmail.com](mailto:emmanuel.cmora20@gmail.com).
"""
